---
title: Tool Calls
subtitle: Using tools in your prompts
---

# Tool Calls

Provide models with new capabilities and data access so they can follow instructions and respond to prompts.

**Tool calls** (also known as **function calling**) provide models with a powerful and flexible way to interface with external systems and access data beyond their training data. This guide will show you how to connect models to data and operations provided by your application. We'll demonstrate how to use tool calls, which can handle free-form text input and output.

How it Works
--------

Let's first understand a few key terms about tool calls. After we have a shared vocabulary understanding of tool calls, we'll walk through some practical examples showing how to implement them.

::: details 1. **Tools** - The capabilities we provide to the model

**Tools** are used to tell the model about a capability it can access. When the model generates a response to a prompt, it might decide it needs the data or functionality provided by a tool to follow the prompt's instructions.

You can provide the model with access to tools like:

*   Get today's weather for a certain location
*   Get account details for a given user ID
*   Issue a refund for a lost order

Or any other operation you want the model to know about or be able to perform when responding to a prompt.

When we send an API request to the model with a prompt, we can include a list of tools the model might consider using. For example, if we want the model to be able to answer questions about the current weather somewhere in the world, we might give it access to a `get_weather` tool that takes `location` as a parameter.

:::

::: details 2. **Tool Calls** - The model's request to use a tool

A **function calling** or **tool calls** refers to a special type of response we get from the model when, after examining the prompt, the model determines that to follow the instructions in the prompt, it needs to call one of the tools we've provided it.

If the model receives a prompt like "What's the weather like in Paris?" in an API request, it can respond with a tool call to the `get_weather` tool with `Paris` as the `location` parameter.

:::

::: details 3. **Tool Calls Output** - The output we generate for the model

**Function call output** or **tool call output** refers to the response generated by the tool using the input from the model's tool call. Tool call output can be structured JSON or plain text, and should include a reference to the specific model tool call (referenced via `tool_call_id` in subsequent examples).

Completing our weather example:

*   The model has access to a `get_weather` **tool** that takes `location` as a parameter.
*   In response to a prompt like "What's the weather like in Paris?", the model returns a **tool call** with a `location` parameter value of `Paris`
*   Our **tool call output** might be JSON structure like `{"temperature": "25", "unit": "C"}`, indicating the current temperature is 25 degrees.

We then send all the tool definitions, original prompt, model's tool call, and tool call output back to the model together, and finally receive a text response like:

```text
The weather in Paris today is 25°C.
```

:::

::: details 4. **Function Tools** vs **Tools**
*   Functions are a specific type of tool defined by JSON Schema. Function definitions allow the model to pass data to your application, and your code can access the data or perform actions suggested by the model.
*   In addition to function tools, there are custom tools that can handle free-form text input and output.
:::

### Tool Call Flow

Tool calling is a multi-turn conversation between your application and the model through the ZenMux API. The tool call flow has five main steps:

1.  Send a request to the model including the tools it can call
2.  Receive tool calls from the model
3.  Execute code on the application side using the tool call inputs
4.  Send a second request to the model including the tool output
5.  Receive the final response from the model (or more tool calls)

<div style="text-align: center;">
  <img src="https://cdn.openai.com/API/docs/images/function-calling-diagram-steps.png" 
       alt="Function calling diagram steps" 
       style="width: 100%; max-width: 600px; border-radius: 8px; margin: 20px 0; background: white; padding: 10px;"
       loading="lazy" />
</div>
> Image referenced from OpenAI

Tool Call Examples
------------

Let's look at a complete tool call flow using `get_horoscope` to get daily horoscope for a zodiac sign.

Complete tool call example:

::: code-group

```Python
from openai import OpenAI
import json

client = OpenAI(
  base_url="https://zenmux.ai/api/v1",
  api_key="<ZENMUX_API_KEY>",
)

# 1. Define list of tools the model can call
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_horoscope",
            "description": "Get today's horoscope for a zodiac sign.",
            "parameters": {
                "type": "object",
                "properties": {
                    "sign": {
                        "type": "string",
                        "description": "The zodiac sign name, such as Taurus or Aquarius",
                    },
                },
                "required": ["sign"],
            },
        },
    },
]

# Create message list, we'll add messages to this over time
message_list = [
    {"role": "user", "content": "What's my horoscope? I'm an Aquarius."}
]

# 2. Prompt the model with the defined tools
response = client.chat.completions.create(
    model="moonshotai/kimi-k2",
    tools=tools,
    messages=message_list,
)

# Save function call output for subsequent requests
function_call = None
function_call_arguments = None
message_list.append({
  "role": "assistant",
  "content": response.choices[0].message.content,
  "tool_calls": [tool_call.model_dump() for tool_call in response.choices[0].message.tool_calls] if response.choices[0].message.tool_calls else None,
})

for item in response.choices[0].message.tool_calls:
    if item.type == "function":
        function_call = item
        function_call_arguments = json.loads(item.function.arguments)

def get_horoscope(sign):
    return f"{sign}: Next Tuesday you will meet a little otter."

# 3. Execute the function logic for get_horoscope
result = {"horoscope": get_horoscope(function_call_arguments["sign"])}

# 4. Provide the function call result to the model
message_list.append({
    "role": "tool",
    "tool_call_id": function_call.id,
    "name": function_call.function.name,
    "content": json.dumps(result),
})

print("Final input:")
print(json.dumps(message_list, indent=2, ensure_ascii=False))

response = client.chat.completions.create(
    model="moonshotai/kimi-k2",
    tools=tools,
    messages=message_list,
)

# 5. The model should now be able to provide a response!
print("Final output:")
print(response.model_dump_json(indent=2))
print("\n" + response.choices[0].message.content)
```

```Typescript
import OpenAI from "openai";
const openai = new OpenAI({
  baseURL: 'https://zenmux.ai/api/v1',
  apiKey: '<ZENMUX_API_KEY>',
});

// 1. Define list of tools the model can call
const tools: OpenAI.Chat.Completions.ChatCompletionTool[] = [
  {
    type: "function",
    function: {
      name: "get_horoscope",
      description: "Get today's horoscope for a zodiac sign.",
      parameters: {
        type: "object",
        properties: {
          sign: {
            type: "string",
            description: "The zodiac sign name, such as Taurus or Aquarius",
          },
        },
        required: ["sign"],
      },
    },
  },
];

// Create message list, we'll add messages to this over time
let input: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
  { role: "user", content: "What's my horoscope? I'm an Aquarius." },
];


async function main() {
  // 2. Use model with tool calling capability
  let response = await openai.chat.completions.create({
    model: "moonshotai/kimi-k2",
    tools,
    messages: input,
  });

  // Save function call output for subsequent requests
  let functionCall: OpenAI.Chat.Completions.ChatCompletionMessageFunctionToolCall | undefined;
  let functionCallArguments: Record<string, string> | undefined;
  input = input.concat(response.choices.map((c) => c.message));

  response.choices.forEach((item) => {
    if (item.message.tool_calls && item.message.tool_calls.length > 0) {
      functionCall = item.message.tool_calls[0] as OpenAI.Chat.Completions.ChatCompletionMessageFunctionToolCall;
      functionCallArguments = JSON.parse(functionCall.function.arguments) as Record<string, string>;
    }
  });

  // 3. Execute the function logic for get_horoscope
  function getHoroscope(sign: string) {
    return sign + " Next Tuesday you will meet a little otter.";
  }

  if (!functionCall || !functionCallArguments) {
    throw new Error("Model didn't return a function call");
  }

  const result = { horoscope: getHoroscope(functionCallArguments.sign) };

  // 4. Provide the function call result to the model
  input.push({
    role: 'tool',
    tool_call_id: functionCall.id,
    // @ts-expect-error must have name
    name: functionCall.function.name,
    content: JSON.stringify(result),
  });
  console.log("Final input:");
  console.log(JSON.stringify(input, null, 2));

  response = await openai.chat.completions.create({
    model: "moonshotai/kimi-k2",
    tools,
    messages: input,
  });

  // 5. The model should now be able to provide a response!
  console.log("Final output:");
  console.log(JSON.stringify(response.choices.map(v => v.message), null, 2));
}

main();
```

:::


::: warning
Note that for reasoning models like GPT-5 or o4-mini, in the final output call, you need to pass the tool call content returned by the model along with the tool call output to the model for summary output.
:::

Defining Function Tools
--------

Function tools can be set in the `tools` parameter. Function tools are defined through their schema, which tells the model what it does and what input parameters it expects. Function tool definitions have the following properties:

|Field|Description|
|---|---|
|type|Should always be function|
|function|Tool structure|
|function.name|Function name (e.g., get_weather)|
|function.description|Details about when and how to use this function|
|function.parameters|JSON Schema defining the function's input parameters|
|function.strict|Whether to enable strict mode adherence when generating function calls|

Here's the definition for a `get_weather` function tool:

```json
{
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "Retrieve the current weather for a given location.",
      "parameters": {
          "type": "object",
          "properties": {
              "location": {
                  "type": "string",
                  "description": "The city and country, e.g. Bogotá, Colombia"
              },
              "units": {
                  "type": "string",
                  "enum": ["celsius", "fahrenheit"],
                  "description": "The unit to return the temperature in."
              }
          },
          "required": ["location", "units"],
          "additionalProperties": false
      },
      "strict": true
    }
}
```

### Token Usage

Under the hood, This `tools` count towards the model's context limit and are billed as prompt tokens. If you run into token limits, we recommend limiting the size and number of `tools`.

Handling Tool Calls
------------

When the model calls a tool from `tools`, you must execute that tool and return the result. Since tool calls can contain zero, one, or multiple calls, the best practice is to assume there will be multiple calls.

The response's `tool_calls` array contains function calls (with `type` as `function`). Each call contains the following fields:

- `id`: Unique identifier for submitting function results later
- `type`: Corresponds to the tool's `type`, generally `function` or `custom`
- `function`: Function structure
    - `name`: Function name
    - `arguments`: JSON-encoded function parameters

Example response with multiple tool calls:

```json
[
    {
        "id": "fc_12345xyz",
        "type": "function",
        "function": {
            "name": "get_weather",
            "arguments": "{\"location\":\"Paris, France\"}"
        }
    },
    {
        "id": "fc_67890abc",
        "type": "function",
        "function": {
            "name": "get_weather",
            "arguments": "{\"location\":\"Bogotá, Colombia\"}"
        }
    },
    {
        "id": "fc_99999def",
        "type": "function",
        "function": {
            "name": "send_email",
            "arguments": "{\"to\":\"bob@email.com\",\"body\":\"Hi bob\"}"
        }
    }
]
```

Execute tool calls and append results:

::: code-group

```Python
for choice in response.choices:
    for tool_call in choice.message.tool_calls or []:
        if tool_call.type != "function":
            continue
        
        name = tool_call.function.name
        args = json.loads(tool_call.function.arguments)
        
        result = call_function(name, args)
        message_list.append({
            "role": "tool",
            "name": name,
            "tool_call_id": tool_call.id,
            "content": str(result)
        })
```

```Typescript
for (const choice of response.choices) {
  for (const toolCall of choice.tool_calls) {
      if (toolCall.type !== "function") {
          continue;
      }

      const name = toolCall.function.name;
      const args = JSON.parse(toolCall.function.arguments);

      const result = callFunction(name, args);
      input.push({
          role: "tool",
          name: name,
          tool_call_id: toolCall.id,
          content: result.toString()
      });
  }
}
```

:::

In the example above, we have a hypothetical `callFunction` to route each call. Here's a possible implementation:

Execute function calls and append results:

::: code-group

```Python
def call_function(name, args):
    if name == "get_weather":
        return get_weather(**args)
    if name == "send_email":
        return send_email(**args)
```

```Typescript
const callFunction = async (name: string, args: unknown) => {
    if (name === "get_weather") {
        return getWeather(args.latitude, args.longitude);
    }
    if (name === "send_email") {
        return sendEmail(args.to, args.body);
    }
};
```

:::

### Formatting Results

Results must be strings, and the content of the string can be freely defined (JSON, error codes, plain text, etc.). The model will interpret the string as needed.

If your tool call has no return value (e.g., `send_email`), simply return a string to represent success or failure. (e.g., `"success"`)

### Incorporating Results into Response

After appending the results to your `input`, you can send them back to the model to get the final response.

Send results back to the model:

::: code-group

```Python
response = client.chat.completions.create(
    model="moonshotai/kimi-k2",
    messages=input_messages,
    tools=tools,
)
```

```javascript
const response = await openai.chat.completions.create({
    model: "moonshotai/kimi-k2",
    messages: input,
    tools,
});
```

:::

Final response:

```json
"Paris is about 15°C, Bogotá is about 18°C, and I've sent that email to Bob."
```

Additional Configuration
--------

### Tool Calling Behavior Control (tool_choice)

By default, the model will determine when and how many tools to use. You can use the `tool_choice` parameter to control the model's tool calling behavior.

1.  **Auto:** (_default_) Call zero, one, or multiple tools. `tool_choice: "auto"`
2.  **Required:** Call one or more tools. `tool_choice: "required"`

**When to use (allowed_tools)**

If you want to use only a subset of your tool list in a model request but don't want to modify the tool list you pass in to maximize prompt caching savings, you can configure `allowed_tools`.

```json
"tool_choice": {
    "type": "allowed_tools",
    "mode": "auto",
    "tools": [
        { "type": "function", "function": { "name": "get_weather" } },
        { "type": "function", "function": { "name": "get_time" } }
    ]
}
```

You can also set `tool_choice` to `"none"` to force the model not to call any tools.

Streaming
--------

Streaming tool calls is very similar to streaming regular responses: you set `stream` to `true` and get a stream of `event` lists.

Streaming tool calls:

::: code-group

```Python
from openai import OpenAI

client = OpenAI(
  base_url="https://zenmux.ai/api/v1",
  api_key="<ZENMUX_API_KEY>",
)

tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get the current temperature for a given location.",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and country, e.g. Bogotá, Colombia"
                }
            },
            "required": [
                "location"
            ],
            "additionalProperties": False
        }
    }
}]

stream = client.chat.completions.create(
    model="moonshotai/kimi-k2",
    messages=[{"role": "user", "content": "How's the weather in Paris today?"}],
    tools=tools,
    stream=True
)

for event in stream:
    print(event.choices[0].delta.model_dump_json())
```

```Typescript
import { OpenAI } from "openai";

const openai = new OpenAI({
  baseURL: 'https://zenmux.ai/api/v1',
  apiKey: '<ZENMUX_API_KEY>',
});

const tools: OpenAI.Chat.Completions.ChatCompletionTool[] = [{
    type: "function",
    function: {
        name: "get_weather",
        description: "Get the current temperature (in Celsius) for the provided coordinates.",
        parameters: {
            type: "object",
            properties: {
                latitude: { type: "number" },
                longitude: { type: "number" }
            },
            required: ["latitude", "longitude"],
            additionalProperties: false
        },
        strict: true,
    },
}];

async function main() {
  const stream = await openai.chat.completions.create({
      model: "moonshotai/kimi-k2",
      messages: [{ role: "user", content: "How's the weather in Paris today?" }],
      tools,
      stream: true,
  });

  for await (const event of stream) {
      console.log(JSON.stringify(event.choices[0].delta));
  }
}

main();
```

:::

Output events:

```json
{"content":"I need","role":"assistant"}
{"content":" the","role":"assistant"}
{"content":" coordinates","role":"assistant"}
{"content":" for","role":"assistant"}
{"content":" Paris","role":"assistant"}
{"content":" to","role":"assistant"}
{"content":" get","role":"assistant"}
{"content":" the","role":"assistant"}
{"content":" weather","role":"assistant"}
{"content":" information","role":"assistant"}
{"content":".","role":"assistant"}
{"content":" Paris","role":"assistant"}
{"content":" has","role":"assistant"}
{"content":" a","role":"assistant"}
{"content":" latitude","role":"assistant"}
{"content":" of","role":"assistant"}
{"content":" approximately","role":"assistant"}
{"content":" ","role":"assistant"}
{"content":"48","role":"assistant"}
{"content":".","role":"assistant"}
{"content":"856","role":"assistant"}
{"content":"6","role":"assistant"}
{"content":" and","role":"assistant"}
{"content":" a","role":"assistant"}
{"content":" longitude","role":"assistant"}
{"content":" of","role":"assistant"}
{"content":" ","role":"assistant"}
{"content":"2","role":"assistant"}
{"content":".","role":"assistant"}
{"content":"352","role":"assistant"}
{"content":"2","role":"assistant"}
{"content":".","role":"assistant"}
{"content":" Let","role":"assistant"}
{"content":" me","role":"assistant"}
{"content":" check","role":"assistant"}
{"content":" the","role":"assistant"}
{"content":" weather","role":"assistant"}
{"content":" for","role":"assistant"}
{"content":" Paris","role":"assistant"}
{"content":" today","role":"assistant"}
{"content":".","role":"assistant"}
{"content":"","role":"assistant","tool_calls":[{"index":0,"id":"get_weather:0","function":{"arguments":"","name":"get_weather"},"type":"function"}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"{\""}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"latitude"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"\":"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":" "}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"48"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"."}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"856"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"6"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":","}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":" \""}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"longitude"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"\":"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":" "}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"2"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"."}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"352"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"2"}}]}
{"content":"","role":"assistant","tool_calls":[{"index":0,"function":{"arguments":"}"}}]}
{"content":"","role":"assistant"}
```

When the model calls one or more tools, an `event` will be output for each tool call with a non-empty `tool_calls.type`:

```json
{"content":"","role":"assistant","tool_calls":[{"index":0,"id":"get_weather:0","function":{"arguments":"","name":"get_weather"},"type":"function"}]}
```

Here's a code snippet demonstrating how to aggregate `delta` into final `tool_call` objects.

Accumulate `tool_call` content:

::: code-group

```Python
final_tool_calls = {}

for event in stream:
    delta = event.choices[0].delta
    if delta.tool_calls and len(delta.tool_calls) > 0:
        tool_call = delta.tool_calls[0]
        if tool_call.type == "function":
            final_tool_calls[tool_call.index] = tool_call
        else:
            final_tool_calls[tool_call.index].function.arguments += tool_call.function.arguments

print("Final tool calls:")
for index, tool_call in final_tool_calls.items():
    print(f"Tool Call {index}:")
    print(tool_call.model_dump_json(indent=2))
```

```Typescript
const finalToolCalls: OpenAI.Chat.Completions.ChatCompletionMessageFunctionToolCall[]  = [];

for await (const event of stream) {
    const delta = event.choices[0].delta;
    if (delta.tool_calls && delta.tool_calls.length > 0) {
        const toolCall = delta.tool_calls[0] as OpenAI.Chat.Completions.ChatCompletionMessageFunctionToolCall & {index: number};
        if (toolCall.type === "function") {
            finalToolCalls[toolCall.index] = toolCall;
        } else {
            finalToolCalls[toolCall.index].function.arguments += toolCall.function.arguments;
        }
    }
}

console.log("Final tool calls:");
console.log(JSON.stringify(finalToolCalls, null, 2));
```

:::

Accumulated final_tool_calls[0]:

```json
{
    "index": 0,
    "id": "get_weather:0",
    "function": {
        "arguments": "{\"latitude\": 48.8566, \"longitude\": 2.3522}",
        "name": "get_weather"
    },
    "type": "function"
}
```
